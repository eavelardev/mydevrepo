{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Open the Dockerfile from the left menu and copy the following into it:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "FROM gcr.io/deeplearning-platform-release/tf2-gpu.2-5\n",
    "WORKDIR /\n",
    "# Installs hypertune library\n",
    "RUN pip install cloudml-hypertune\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY trainer /trainer\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Next, open the task.py file you just created and copy the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import argparse\n",
    "import hypertune\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "def get_args():\n",
    "  '''Parses args. Must include all hyperparameters you want to tune.'''\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\n",
    "      '--learning_rate',\n",
    "      required=True,\n",
    "      type=float,\n",
    "      help='learning rate')\n",
    "  parser.add_argument(\n",
    "      '--momentum',\n",
    "      required=True,\n",
    "      type=float,\n",
    "      help='SGD momentum value')\n",
    "  parser.add_argument(\n",
    "      '--num_neurons',\n",
    "      required=True,\n",
    "      type=int,\n",
    "      help='number of units in last hidden layer')\n",
    "  args = parser.parse_args()\n",
    "  return args\n",
    "\n",
    "def preprocess_data(image, label):\n",
    "  '''Resizes and scales images.'''\n",
    "  image = tf.image.resize(image, (150,150))\n",
    "  return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "def create_dataset():\n",
    "  '''Loads Horses Or Humans dataset and preprocesses data.'''\n",
    "  data, _ = tfds.load(name='horses_or_humans', as_supervised=True, with_info=True)\n",
    "  \n",
    "  # Create train dataset\n",
    "  train_data = data['train'].map(preprocess_data)\n",
    "  train_data  = train_data.shuffle(1000)\n",
    "  train_data  = train_data.batch(64)\n",
    "  \n",
    "  # Create validation dataset\n",
    "  validation_data = data['test'].map(preprocess_data)\n",
    "  validation_data  = validation_data.batch(64)\n",
    "  return train_data, validation_data\n",
    "\n",
    "def create_model(num_neurons, learning_rate, momentum):\n",
    "  '''Defines and complies model.'''\n",
    "  inputs = tf.keras.Input(shape=(150, 150, 3))\n",
    "  x = tf.keras.layers.Conv2D(16, (3, 3), activation='relu')(inputs)\n",
    "  x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "  x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu')(x)\n",
    "  x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "  x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "  x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "  x = tf.keras.layers.Flatten()(x)\n",
    "  x = tf.keras.layers.Dense(num_neurons, activation='relu')(x)\n",
    "  outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "  model = tf.keras.Model(inputs, outputs)\n",
    "  model.compile(\n",
    "      loss='binary_crossentropy',\n",
    "      optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum),\n",
    "      metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def main():\n",
    "  args = get_args()\n",
    "  train_data, validation_data = create_dataset()\n",
    "  model = create_model(args.num_neurons, args.learning_rate, args.momentum)\n",
    "  history = model.fit(train_data, epochs=NUM_EPOCHS, validation_data=validation_data)\n",
    "  \n",
    "  # DEFINE METRIC\n",
    "  hp_metric = history.history['val_accuracy'][-1]\n",
    "  hpt = hypertune.HyperTune()\n",
    "  hpt.report_hyperparameter_tuning_metric(\n",
    "      hyperparameter_metric_tag='accuracy',\n",
    "      metric_value=hp_metric,\n",
    "      global_step=NUM_EPOCHS)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PROJECT_ID='your-cloud-project'  # gcloud config list --format 'value(core.project)\n",
    "IMAGE_URI=\"gcr.io/$PROJECT_ID/horse-human:hypertune\"\n",
    "docker build ./ -t $IMAGE_URI\n",
    "docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install google-cloud-aiplatform --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. To launch the hyperparameter tuning job, you need to first define the following specs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The spec of the worker pools including machine type and Docker image\n",
    "# Be sure to replace IMAGE_URI with the path to your Docker image in GCR\n",
    "worker_pool_specs = [{\n",
    "    \"machine_spec\": {\n",
    "        \"machine_type\": \"n1-standard-4\",\n",
    "        \"accelerator_type\": \"NVIDIA_TESLA_T4\",\n",
    "        \"accelerator_count\": 1\n",
    "    },\n",
    "    \"replica_count\": 1,\n",
    "    \"container_spec\": {\n",
    "        \"image_uri\": \"gcr.io/{IMAGE_URI}\"   # Replace the {IMAGE_URI} with your Docker image path\n",
    "    }\n",
    "}]\n",
    "\n",
    "# Dicionary representing metrics to optimize.\n",
    "# The dictionary key is the metric_id, which is reported by your training job,\n",
    "# And the dictionary value is the optimization goal of the metric.\n",
    "metric_spec={'accuracy':'maximize'}\n",
    "\n",
    "# Dictionary representing parameters to optimize.\n",
    "# The dictionary key is the parameter_id, which is passed into your training\n",
    "# job as a command line argument,\n",
    "# And the dictionary value is the parameter specification of the metric.\n",
    "parameter_spec = {\n",
    "    \"learning_rate\": hpt.DoubleParameterSpec(min=0.001, max=1, scale=\"log\"),\n",
    "    \"momentum\": hpt.DoubleParameterSpec(min=0, max=1, scale=\"linear\"),\n",
    "    \"num_neurons\": hpt.DiscreteParameterSpec(values=[64, 128, 512], scale=None)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Now, create a new bucket and copy all the content from your old bucket to the new one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new bucket in us-central1 region\n",
    "!gsutil mb -p <PROJECT_ID> -l us-central1 -b on gs://<PROJECT_ID>  # Replace <PROJECT_ID> with your project id\n",
    "# Copy the content in newly created bucket from the old one\n",
    "!gsutil cp -r gs://artifacts.<PROJECT_ID>.appspot.com/* gs://<PROJECT_ID>  # Replace <PROJECT_ID> with your project id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Next, create a CustomJob. You'll need to replace {YOUR_BUCKET} with a bucket in your project for staging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace (YOUR_BUCKET} with your project ID\n",
    "my_custom_job = aiplatform.CustomJob(display_name='horses-humans-sdk-job',\n",
    "                              worker_pool_specs=worker_pool_specs,\n",
    "                              staging_bucket='gs://{YOUR_BUCKET}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Then, create and run the HyperparameterTuningJob:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_job = aiplatform.HyperparameterTuningJob(\n",
    "    display_name='horses-humans-sdk-job',\n",
    "    custom_job=my_custom_job,\n",
    "    metric_spec=metric_spec,\n",
    "    parameter_spec=parameter_spec,\n",
    "    max_trial_count=15,\n",
    "    parallel_trial_count=3)\n",
    "    \n",
    "hp_job.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml_dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3dbb6e28f32ca2376e4f57c81cef85cf88ffd88cbd1c487658c99f00bdea0c93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
