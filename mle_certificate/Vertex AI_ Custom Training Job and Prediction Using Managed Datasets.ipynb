{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Copy-paste the following code in titanic/trainer/task.py. The code contains comments, so it will help to spend a few minutes going through the file to better understand it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery, bigquery_storage, storage\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from typing import Union, List\n",
    "import os, logging, json, pickle, argparse\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# feature selection. The FEATURE list defines what features are needed from the training data\n",
    "# as well as the types of those features. We will perform different feature engineering depending on the type.\n",
    "# List all column names for binary features: 0,1 or True,False or Male,Female etc\n",
    "BINARY_FEATURES = [\n",
    "    'sex']\n",
    "\n",
    "# List all column names for numeric features\n",
    "NUMERIC_FEATURES = [\n",
    "    'age',\n",
    "    'fare']\n",
    "\n",
    "# List all column names for categorical features\n",
    "CATEGORICAL_FEATURES = [\n",
    "    'pclass',\n",
    "    'embarked',\n",
    "    'home_dest',\n",
    "    'parch',\n",
    "    'sibsp']\n",
    "ALL_COLUMNS = BINARY_FEATURES+NUMERIC_FEATURES+CATEGORICAL_FEATURES\n",
    "\n",
    "# define the column name for label\n",
    "LABEL = 'survived'\n",
    "\n",
    "# Define the index position of each feature. This is needed for processing a\n",
    "# numpy array (instead of pandas) which has no column names.\n",
    "BINARY_FEATURES_IDX = list(range(0,len(BINARY_FEATURES)))\n",
    "NUMERIC_FEATURES_IDX = list(range(len(BINARY_FEATURES), len(BINARY_FEATURES)+len(NUMERIC_FEATURES)))\n",
    "CATEGORICAL_FEATURES_IDX = list(range(len(BINARY_FEATURES+NUMERIC_FEATURES), len(ALL_COLUMNS)))\n",
    "\n",
    "def load_data_from_gcs(data_gcs_path: str) -> pd.DataFrame:\n",
    "    '''\n",
    "    Loads data from Google Cloud Storage (GCS) to a dataframe\n",
    "            Parameters:\n",
    "                    data_gcs_path (str): gs path for the location of the data. Wildcards are also supported. i.e gs://example_bucket/data/training-*.csv\n",
    "            Returns:\n",
    "                    pandas.DataFrame: a dataframe with the data from GCP loaded\n",
    "    '''\n",
    "    # using dask that supports wildcards to read multiple files. Then with dd.read_csv().compute we create a pandas dataframe\n",
    "    # Additionally I have noticed that some values for TotalCharges are missing and this creates confusion regarding TotalCharges as the data type.\n",
    "    # to overcome this we manually define TotalCharges as object.\n",
    "    # We will later fix this abnormality\n",
    "    logging.info(\"reading gs data: {}\".format(data_gcs_path))\n",
    "    return dd.read_csv(data_gcs_path, dtype={'TotalCharges': 'object'}).compute()\n",
    "\n",
    "def load_data_from_bq(bq_uri: str) -> pd.DataFrame:\n",
    "    '''\n",
    "    Loads data from BigQuery table (BQ) to a dataframe\n",
    "            Parameters:\n",
    "                    bq_uri (str): bq table uri. i.e: example_project.example_dataset.example_table\n",
    "            Returns:\n",
    "                    pandas.DataFrame: a dataframe with the data from GCP loaded\n",
    "    '''\n",
    "    if not bq_uri.startswith('bq://'):\n",
    "        raise Exception(\"uri is not a BQ uri. It should be bq://project_id.dataset.table\")\n",
    "    logging.info(\"reading bq data: {}\".format(bq_uri))\n",
    "    project,dataset,table =  bq_uri.split(\".\")\n",
    "    bqclient = bigquery.Client(project=project[5:])\n",
    "    bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "    query_string = \"\"\"\n",
    "    SELECT * from {ds}.{tbl}\n",
    "    \"\"\".format(ds=dataset, tbl=table)\n",
    "    return (\n",
    "        bqclient.query(query_string)\n",
    "        .result()\n",
    "        .to_dataframe(bqstorage_client=bqstorageclient)\n",
    "    )\n",
    "\n",
    "def clean_missing_numerics(df: pd.DataFrame, numeric_columns):\n",
    "    '''\n",
    "    removes invalid values in the numeric columns        \n",
    "            Parameters:\n",
    "                    df (pandas.DataFrame): The Pandas Dataframe to alter\n",
    "                    numeric_columns (List[str]): List of column names that are numeric from the DataFrame\n",
    "            Returns:\n",
    "                    pandas.DataFrame: a dataframe with the numeric columns fixed\n",
    "    '''\n",
    "    for n in numeric_columns:\n",
    "        df[n] = pd.to_numeric(df[n], errors='coerce')\n",
    "    df = df.fillna(df.mean())\n",
    "    return df\n",
    "\n",
    "def data_selection(df: pd.DataFrame, selected_columns: List[str], label_column: str) -> (pd.DataFrame, pd.Series):\n",
    "    '''\n",
    "    From a dataframe it creates a new dataframe with only selected columns and returns it.\n",
    "    Additionally it splits the label column into a pandas Series.\n",
    "            Parameters:\n",
    "                    df (pandas.DataFrame): The Pandas Dataframe to drop columns and extract label\n",
    "                    selected_columns (List[str]): List of strings with the selected columns. i,e ['col_1', 'col_2', ..., 'col_n' ]\n",
    "                    label_column (str): The name of the label column\n",
    "            Returns:\n",
    "                    tuple(pandas.DataFrame, pandas.Series): Tuble with the new pandas DataFrame containing only selected columns and lablel pandas Series\n",
    "    '''\n",
    "    # We create a series with the prediciton label\n",
    "    labels = df[label_column]\n",
    "    data = df.loc[:, selected_columns]\n",
    "    return data, labels\n",
    "\n",
    "def pipeline_builder(params_svm: dict, bin_ftr_idx: List[int], num_ftr_idx: List[int], cat_ftr_idx: List[int]) -> Pipeline:\n",
    "    '''\n",
    "    Builds a sklearn pipeline with preprocessing and model configuration.\n",
    "    Preprocessing steps are:\n",
    "        * OrdinalEncoder - used for binary features\n",
    "        * StandardScaler - used for numerical features\n",
    "        * OneHotEncoder - used for categorical features\n",
    "    Model used is SVC\n",
    "            Parameters:\n",
    "                    params_svm (dict): List of parameters for the sklearn.svm.SVC classifier\n",
    "                    bin_ftr_idx (List[str]): List of ints that mark the column indexes with binary columns. i.e [0, 2, ... , X ]\n",
    "                    num_ftr_idx (List[str]): List of ints that mark the column indexes with numerical columns. i.e [6, 3, ... , X ]\n",
    "                    cat_ftr_idx (List[str]): List of ints that mark the column indexes with categorical columns. i.e [5, 10, ... , X ]\n",
    "                    label_column (str): The name of the label column\n",
    "            Returns:\n",
    "                     Pipeline: sklearn.pipelines.Pipeline with preprocessing and model training\n",
    "    '''\n",
    "    # Defining a preprocessing step for our pipeline.\n",
    "    # it specifies how the features are going to be transformed\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('bin', OrdinalEncoder(), bin_ftr_idx),\n",
    "            ('num', StandardScaler(), num_ftr_idx),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), cat_ftr_idx)], n_jobs=-1)\n",
    "\n",
    "    # We now create a full pipeline, for preprocessing and training.\n",
    "    # for training we selected a linear SVM classifier\n",
    "    clf = SVC()\n",
    "    clf.set_params(**params_svm)\n",
    "    return Pipeline(steps=[ ('preprocessor', preprocessor),\n",
    "                          ('classifier', clf)])\n",
    "def train_pipeline(clf: Pipeline, X: Union[pd.DataFrame, np.ndarray], y: Union[pd.DataFrame, np.ndarray]) -> float:\n",
    "    '''\n",
    "    Trains a sklearn pipeline by fiting training data and labels and returns the accuracy f1 score\n",
    "            Parameters:\n",
    "                    clf (sklearn.pipelines.Pipeline): the Pipeline object to fit the data\n",
    "                    X: (pd.DataFrame OR np.ndarray): Training vectors of shape n_samples x n_features, where n_samples is the number of samples and n_features is the number of features.\n",
    "                    y: (pd.DataFrame OR np.ndarray): Labels of shape n_samples. Order should mathc Training Vectors X\n",
    "            Returns:\n",
    "                    score (float): Average F1 score from all cross validations\n",
    "    '''\n",
    "    # run cross validation to get training score. we can use this score to optimize training\n",
    "    score = cross_val_score(clf, X, y, cv=10, n_jobs=-1).mean()\n",
    "\n",
    "    # Now we fit all our data to the classifier.\n",
    "    clf.fit(X, y)\n",
    "    return score\n",
    "\n",
    "def process_gcs_uri(uri: str) -> (str, str, str, str):\n",
    "    '''\n",
    "    Receives a Google Cloud Storage (GCS) uri and breaks it down to the scheme, bucket, path and file\n",
    "            Parameters:\n",
    "                    uri (str): GCS uri\n",
    "            Returns:\n",
    "                    scheme (str): uri scheme\n",
    "                    bucket (str): uri bucket\n",
    "                    path (str): uri path\n",
    "                    file (str): uri file\n",
    "    '''\n",
    "    url_arr = uri.split(\"/\")\n",
    "    if \".\" not in url_arr[-1]:\n",
    "        file = \"\"\n",
    "    else:\n",
    "        file = url_arr.pop()\n",
    "    scheme = url_arr[0]\n",
    "    bucket = url_arr[2]\n",
    "    path = \"/\".join(url_arr[3:])\n",
    "    path = path[:-1] if path.endswith(\"/\") else path\n",
    "    return scheme, bucket, path, file\n",
    "\n",
    "def pipeline_export_gcs(fitted_pipeline: Pipeline, model_dir: str) -> str:\n",
    "    '''\n",
    "    Exports trained pipeline to GCS\n",
    "            Parameters:\n",
    "                    fitted_pipeline (sklearn.pipelines.Pipeline): the Pipeline object with data already fitted (trained pipeline object)\n",
    "                    model_dir (str): GCS path to store the trained pipeline. i.e gs://example_bucket/training-job\n",
    "            Returns:\n",
    "                    export_path (str): Model GCS location\n",
    "    '''\n",
    "    scheme, bucket, path, file = process_gcs_uri(model_dir)\n",
    "    if scheme != \"gs:\":\n",
    "            raise ValueError(\"URI scheme must be gs\")\n",
    "\n",
    "    # Upload the model to GCS\n",
    "    b = storage.Client().bucket(bucket)\n",
    "    export_path = os.path.join(path, 'model.pkl')\n",
    "    blob = b.blob(export_path)\n",
    "    blob.upload_from_string(pickle.dumps(fitted_pipeline))\n",
    "    return scheme + \"//\" + os.path.join(bucket, export_path)\n",
    "\n",
    "def prepare_report(cv_score: float, model_params: dict, classification_report: str, columns: List[str], example_data: np.ndarray) -> str:\n",
    "    '''\n",
    "    Prepares a training report in Text\n",
    "            Parameters:\n",
    "                    cv_score (float): score of the training job during cross validation of training data\n",
    "                    model_params (dict): dictonary containing the parameters the model was trained with\n",
    "                    classification_report (str): Model classification report with test data\n",
    "                    columns (List[str]): List of columns that where used in training.\n",
    "                    example_data (np.array): Sample of data (2-3 rows are enough). This is used to include what the prediciton payload should look like for the model\n",
    "            Returns:\n",
    "                    report (str): Full report in text\n",
    "    '''\n",
    "    buffer_example_data = '['\n",
    "    for r in example_data:\n",
    "        buffer_example_data+='['\n",
    "        for c in r:\n",
    "            if(isinstance(c,str)):\n",
    "                buffer_example_data+=\"'\"+c+\"', \"\n",
    "            else:\n",
    "                buffer_example_data+=str(c)+\", \"\n",
    "        buffer_example_data= buffer_example_data[:-2]+\"], \\n\"\n",
    "    buffer_example_data= buffer_example_data[:-3]+\"]\"\n",
    "    report = \"\"\"\n",
    "Training Job Report    \n",
    "Cross Validation Score: {cv_score}\n",
    "Training Model Parameters: {model_params}\n",
    "Test Data Classification Report:\n",
    "{classification_report}\n",
    "Example of data array for prediciton:\n",
    "Order of columns:\n",
    "{columns}\n",
    "Example for clf.predict()\n",
    "{predict_example}\n",
    "Example of GCP API request body:\n",
    "{{\n",
    "    \"instances\": {json_example}\n",
    "}}\n",
    "\"\"\".format(\n",
    "    cv_score=cv_score,\n",
    "    model_params=json.dumps(model_params),\n",
    "    classification_report=classification_report,\n",
    "    columns = columns,\n",
    "    predict_example = buffer_example_data,\n",
    "    json_example = json.dumps(example_data.tolist()))\n",
    "    return report\n",
    "\n",
    "def report_export_gcs(report: str, report_dir: str) -> None:\n",
    "    '''\n",
    "    Exports training job report to GCS\n",
    "            Parameters:\n",
    "                    report (str): Full report in text to sent to GCS\n",
    "                    report_dir (str): GCS path to store the report model. i.e gs://example_bucket/training-job\n",
    "            Returns:\n",
    "                    export_path (str): Report GCS location\n",
    "    '''\n",
    "    scheme, bucket, path, file = process_gcs_uri(report_dir)\n",
    "    if scheme != \"gs:\":\n",
    "            raise ValueError(\"URI scheme must be gs\")\n",
    "\n",
    "    # Upload the model to GCS\n",
    "    b = storage.Client().bucket(bucket)\n",
    "    export_path = os.path.join(path, 'report.txt')\n",
    "    blob = b.blob(export_path)\n",
    "    blob.upload_from_string(report)\n",
    "    return scheme + \"//\" + os.path.join(bucket, export_path)\n",
    "\n",
    "# Define all the command-line arguments your model can accept for training\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Input Arguments\n",
    "    parser.add_argument(\n",
    "        '--model_param_kernel',\n",
    "        help = 'SVC model parameter- kernel',\n",
    "        choices=['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "        type = str,\n",
    "        default = 'linear'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--model_param_degree',\n",
    "        help = 'SVC model parameter- Degree. Only applies for poly kernel',\n",
    "        type = int,\n",
    "        default = 3\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--model_param_C',\n",
    "        help = 'SVC model parameter- C (regularization)',\n",
    "        type = float,\n",
    "        default = 1.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--model_param_probability',\n",
    "        help = 'Whether to enable probability estimates',\n",
    "        type = bool,\n",
    "        default = True\n",
    "    )\n",
    "    '''\n",
    "    Vertex AI automatically populates a set of environment variables in the container that executes\n",
    "    your training job. Those variables include:\n",
    "        * AIP_MODEL_DIR - Directory selected as model dir\n",
    "        * AIP_DATA_FORMAT - Type of dataset selected for training (can be csv or bigquery)\n",
    "    Vertex AI will automatically split selected dataset into training, validation and testing\n",
    "    and 3 more environment variables will reflect the location of the data:\n",
    "        * AIP_TRAINING_DATA_URI - URI of Training data\n",
    "        * AIP_VALIDATION_DATA_URI - URI of Validation data\n",
    "        * AIP_TEST_DATA_URI - URI of Test data\n",
    "    Notice that those environment variables are default. If the user provides a value using CLI argument,\n",
    "    the environment variable will be ignored. If the user does not provide anything as CLI argument\n",
    "    the program will try and use the environment variables if those exist. Otherwise will leave empty.\n",
    "    '''   \n",
    "    parser.add_argument(\n",
    "        '--model_dir',\n",
    "        help = 'Directory to output model and artifacts',\n",
    "        type = str,\n",
    "        default = os.environ['AIP_MODEL_DIR'] if 'AIP_MODEL_DIR' in os.environ else \"\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--data_format',\n",
    "        choices=['csv', 'bigquery'],\n",
    "        help = 'format of data uri csv for gs:// paths and bigquery for project.dataset.table formats',\n",
    "        type = str,\n",
    "        default =  os.environ['AIP_DATA_FORMAT'] if 'AIP_DATA_FORMAT' in os.environ else \"csv\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--training_data_uri',\n",
    "        help = 'location of training data in either gs:// uri or bigquery uri',\n",
    "        type = str,\n",
    "        default =  os.environ['AIP_TRAINING_DATA_URI'] if 'AIP_TRAINING_DATA_URI' in os.environ else \"\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--validation_data_uri',\n",
    "        help = 'location of validation data in either gs:// uri or bigquery uri',\n",
    "        type = str,\n",
    "        default =  os.environ['AIP_VALIDATION_DATA_URI'] if 'AIP_VALIDATION_DATA_URI' in os.environ else \"\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--test_data_uri',\n",
    "        help = 'location of test data in either gs:// uri or bigquery uri',\n",
    "        type = str,\n",
    "        default =  os.environ['AIP_TEST_DATA_URI'] if 'AIP_TEST_DATA_URI' in os.environ else \"\"\n",
    "    )\n",
    "    parser.add_argument(\"-v\", \"--verbose\", help=\"increase output verbosity\",\n",
    "                    action=\"store_true\")\n",
    "    args = parser.parse_args()\n",
    "    arguments = args.__dict__\n",
    "\n",
    "    if args.verbose:\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    logging.info('Model artifacts will be exported here: {}'.format(arguments['model_dir']))\n",
    "    logging.info('Data format: {}'.format(arguments[\"data_format\"]))\n",
    "    logging.info('Training data uri: {}'.format(arguments['training_data_uri']) )\n",
    "    logging.info('Validation data uri: {}'.format(arguments['validation_data_uri']))\n",
    "    logging.info('Test data uri: {}'.format(arguments['test_data_uri']))\n",
    "    '''\n",
    "    We have 2 different ways to load our data to pandas. One is from Cloud Storage by loading csv files and\n",
    "    the other is by connecting to BigQuery. Vertex AI supports both and\n",
    "    here we created a code that depending on the dataset provided. We will select the appropriate loading method.\n",
    "    '''\n",
    "    logging.info('Loading {} data'.format(arguments[\"data_format\"]))\n",
    "\n",
    "    if(arguments['data_format']=='csv'):\n",
    "        df_train = load_data_from_gcs(arguments['training_data_uri'])\n",
    "        df_test = load_data_from_bq(arguments['test_data_uri'])\n",
    "        df_valid = load_data_from_gcs(arguments['validation_data_uri'])\n",
    "    elif(arguments['data_format']=='bigquery'):\n",
    "        print(arguments['training_data_uri'])\n",
    "        df_train = load_data_from_bq(arguments['training_data_uri'])\n",
    "        df_test = load_data_from_bq(arguments['test_data_uri'])\n",
    "        df_valid = load_data_from_bq(arguments['validation_data_uri'])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid data type \")\n",
    "        \n",
    "    #as we will be using cross validation, we will have just a training set and a single test set.\n",
    "    # we will merge the test and validation to achieve an 80%-20% split\n",
    "    df_test = pd.concat([df_test,df_valid])\n",
    "    logging.info('Defining model parameters')    \n",
    "    model_params = dict()\n",
    "    model_params['kernel'] = arguments['model_param_kernel']\n",
    "    model_params['degree'] = arguments['model_param_degree']\n",
    "    model_params['C'] = arguments['model_param_C']\n",
    "    model_params['probability'] = arguments['model_param_probability']\n",
    "    df_train = clean_missing_numerics(df_train, NUMERIC_FEATURES)\n",
    "    df_test = clean_missing_numerics(df_test, NUMERIC_FEATURES)\n",
    "    logging.info('Running feature selection')    \n",
    "    X_train, y_train = data_selection(df_train, ALL_COLUMNS, LABEL)\n",
    "    X_test, y_test = data_selection(df_test, ALL_COLUMNS, LABEL)\n",
    "    logging.info('Training pipelines in CV')   \n",
    "    clf = pipeline_builder(model_params, BINARY_FEATURES_IDX, NUMERIC_FEATURES_IDX, CATEGORICAL_FEATURES_IDX)\n",
    "    cv_score = train_pipeline(clf, X_train, y_train)\n",
    "    logging.info('Export trained pipeline and report')   \n",
    "    pipeline_export_gcs(clf, arguments['model_dir'])\n",
    "    y_pred = clf.predict(X_test)\n",
    "    test_score = f1_score(y_test, y_pred, average='weighted')\n",
    "    logging.info('f1score: '+ str(test_score))    \n",
    "    \n",
    "    report = prepare_report(cv_score,\n",
    "                        model_params,\n",
    "                        classification_report(y_test,y_pred),\n",
    "                        ALL_COLUMNS,\n",
    "                        X_test.to_numpy()[0:2])\n",
    "\n",
    "    report_export_gcs(report, arguments['model_dir'])\n",
    "    logging.info('Training job completed. Exiting...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Copy-paste the following code in titanic/setup.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "REQUIRED_PACKAGES = [\n",
    "    'gcsfs==0.7.1',\n",
    "    'dask[dataframe]==2021.2.0',\n",
    "    'google-cloud-bigquery-storage==1.0.0',\n",
    "    'six==1.15.0'\n",
    "]\n",
    "setup(\n",
    "    name='trainer',\n",
    "    version='0.1',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(), # Automatically find packages within this directory or below.\n",
    "    include_package_data=True, # if packages include any data files, those will be packed together.\n",
    "    description='Classification training titanic survivors prediction model'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml_dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3dbb6e28f32ca2376e4f57c81cef85cf88ffd88cbd1c487658c99f00bdea0c93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
